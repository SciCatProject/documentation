<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. Introduction</a></li>
<li><a href="#sec-2">2. General preconditions</a>
<ul>
<li><a href="#sec-2-1">2.1. Online work stations in beamline hutches</a></li>
<li><a href="#sec-2-2">2.2. Ra Cluster</a></li>
<li><a href="#sec-2-3">2.3. Windows systems:</a></li>
<li><a href="#sec-2-4">2.4. Other systems:</a></li>
</ul>
</li>
<li><a href="#sec-3">3. CLI Command line interface</a>
<ul>
<li><a href="#sec-3-1">3.1. Quick Overview of Workflow</a>
<ul>
<li><a href="#sec-3-1-1">3.1.1. Ingest the dataset into the Data Catalog.</a></li>
<li><a href="#sec-3-1-2">3.1.2. Archive the dataset</a></li>
<li><a href="#sec-3-1-3">3.1.3. Retrieve data from the catalog</a></li>
</ul>
</li>
<li><a href="#sec-3-2">3.2. Ingestion of Datasets</a>
<ul>
<li><a href="#sec-3-2-1">3.2.1. Introduction and Overview</a></li>
<li><a href="#sec-3-2-2">3.2.2. Definition of input files</a></li>
<li><a href="#sec-3-2-3">3.2.3. Recommended file structure for raw datasets</a></li>
<li><a href="#sec-3-2-4">3.2.4. Definition of metadata</a></li>
<li><a href="#sec-3-2-5">3.2.5. Use Case: Manual ingest using datasetIngestor program</a></li>
<li><a href="#sec-3-2-6">3.2.6. Use Case: Automated ingest of raw datasets from beamline or instruments</a></li>
<li><a href="#sec-3-2-7">3.2.7. Use Case: Ingest datasets stored on decentral systems</a></li>
<li><a href="#sec-3-2-8">3.2.8. Use Case: Ingest datasets from simulations/model calculations</a></li>
</ul>
</li>
<li><a href="#sec-3-3">3.3. Archiving of Datasets</a></li>
<li><a href="#sec-3-4">3.4. Retrieval of Datasets</a>
<ul>
<li><a href="#sec-3-4-1">3.4.1. Overview of Retrieval</a></li>
<li><a href="#sec-3-4-2">3.4.2. More detailed description</a></li>
<li><a href="#sec-3-4-3">3.4.3. Expert commands</a></li>
</ul>
</li>
<li><a href="#sec-3-5">3.5. Analyzing Metadata Statistics</a>
<ul>
<li><a href="#sec-3-5-1">3.5.1. Overview</a></li>
<li><a href="#sec-3-5-2">3.5.2. Getting started</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-4">4. Graphical desktop user interface - SciCat</a>
<ul>
<li><a href="#sec-4-1">4.1. General considerations</a></li>
<li><a href="#sec-4-2">4.2. Getting started</a></li>
<li><a href="#sec-4-3">4.3. Login and permissions</a></li>
<li><a href="#sec-4-4">4.4. Pgroup selection</a></li>
<li><a href="#sec-4-5">4.5. Archiving</a></li>
<li><a href="#sec-4-6">4.6. Retrieval</a></li>
<li><a href="#sec-4-7">4.7. Settings</a></li>
</ul>
</li>
<li><a href="#sec-5">5. Policy settings and email notifications</a></li>
<li><a href="#sec-6">6. Publication of Data</a></li>
<li><a href="#sec-7">7. Policy Parameters</a></li>
<li><a href="#sec-8">8. Appendix</a>
<ul>
<li><a href="#sec-8-1">8.1. Access to the API (for script developers)</a></li>
<li><a href="#sec-8-2">8.2. Using datasetIngestor inside wrapper scripts (for developers)</a></li>
<li><a href="#sec-8-3">8.3. Ingestion of datasets which should never be published</a></li>
<li><a href="#sec-8-4">8.4. Retrieving proposal information</a></li>
<li><a href="#sec-8-5">8.5. Link to Group specific descriptions</a></li>
<li><a href="#sec-8-6">8.6. Metadata Field definitions</a>
<ul>
<li><a href="#sec-8-6-1">8.6.1. Metadata field definitions for datasets of type "base"</a></li>
<li><a href="#sec-8-6-2">8.6.2. Additional fields for type="raw"</a></li>
<li><a href="#sec-8-6-3">8.6.3. Additional fields for type="derived"</a></li>
</ul>
</li>
<li><a href="#sec-8-7">8.7. Instructions to set ACLS in AFS</a></li>
<li><a href="#sec-8-8">8.8. List of known creationLocation for raw data</a></li>
<li><a href="#sec-8-9">8.9. SLS</a></li>
<li><a href="#sec-8-10">8.10. Swissfel</a></li>
<li><a href="#sec-8-11">8.11. SINQ</a></li>
<li><a href="#sec-8-12">8.12. SmuS</a></li>
<li><a href="#sec-8-13">8.13. Size limitations</a></li>
</ul>
</li>
<li><a href="#sec-9">9. Troubleshooting</a>
<ul>
<li><a href="#sec-9-1">9.1. Locale error message</a></li>
</ul>
</li>
<li><a href="#sec-10">10. Update History of Ingest Manual</a></li>
</ul>
</div>
</div>


# Introduction<a id="sec-1" name="sec-1"></a>

PSI provides access to the Data Catalog for long-term data storage and
retrieval. Data is stored on the PetaByte Archive at the Swiss
National Supercomputing Centre (CSCS).  The Data Catalog and Archive
is suitable for:

-   Raw data generated by PSI instruments or simulations
-   Derived data produced by processing the raw input data
-   Data required to reproduce PSI research and publications

The service is based on the catalog system SciCat
<https://github.com/SciCatProject/>, which is an open system that allows
to ingest and retrieve datasets in different ways, matching the
requirements of the respective use cases. The methods differ in the
level of automation provided.

The following sections describe the main principal steps for using the
Data Catalog.

# General preconditions<a id="sec-2" name="sec-2"></a>

All the following assumes that you have access to **Linux** or **Windows**
based system at PSI. The main command line tools are called
"datasetIngestor" , "datasetArchiver" and "datasetRetriever" (Linux
only for now).

For the access to the SciCat web-based user interface no software
needs to be installed, simply use your browser to go to
<https://discovery.psi.ch>. 

Additionally **SciCat**, a graphical desktop user interface based on Qt is
available on **Linux** and **Windows** based systems at PSI. For using the
command line tools and the graphical user interface, you may need to
first install them.

## Online work stations in beamline hutches<a id="sec-2-1" name="sec-2-1"></a>

The command line tools are pre-installed in /work/sls/bin. No further action needed

## Ra Cluster<a id="sec-2-2" name="sec-2-2"></a>

Use the following command to get access to the software

    module load datacatalog

## Windows systems:<a id="sec-2-3" name="sec-2-3"></a>

For Windows Systems you can simply download the binaries from the
following locations. But make sure that you execute the command line
tools inside a **powershell**

    https://gitlab.psi.ch/scicat/tools/raw/master/windows/datasetIngestor.exe
    https://gitlab.psi.ch/scicat/tools/raw/master/windows/SciCat.exe

For Windows systems you can only use personal accounts and the data is
always handled as "decentral" case, i.e. the data will first be copied from
the windows machine to a central file server via scp first.

Please also note the syntax, that has to be used for the definition of
the sourceFolder inside the metadata.json file: this has to be in the
following form:

    "sourceFolder": "/C/Somefolder/etc",

, i.e. **forward slashes** and **no colon** ":" after the drive letter like
"C:" in this case.

The dataset retrieval **to** Windows system is not yet implemented
(WIP). This means for now, that you can archive data from a Windows
machine, but you need to retrieve it back to a Linux system.

## Other systems:<a id="sec-2-4" name="sec-2-4"></a>

For other Linux systems you can fetch the program most easily through the
following command, which copies the executable from a central
webserver

    /usr/bin/curl -O https://gitlab.psi.ch/scicat/tools/raw/master/linux/datasetIngestor;chmod +x ./datasetIngestor
    /usr/bin/curl -O https://gitlab.psi.ch/scicat/tools/raw/master/linux/datasetArchiver;chmod +x ./datasetArchiver
    /usr/bin/curl -O https://gitlab.psi.ch/scicat/tools/raw/master/linux/datasetRetriever;chmod +x ./datasetRetriever

# CLI Command line interface<a id="sec-3" name="sec-3"></a>

## Quick Overview of Workflow<a id="sec-3-1" name="sec-3-1"></a>

### Ingest the dataset into the Data Catalog.<a id="sec-3-1-1" name="sec-3-1-1"></a>

This makes the data known to the Data Catalog system at PSI.

-   Prepare a metadata file describing the dataset
-   run **datasetIngestor** script
-   If necessary, the script will copy the data to the PSI archive servers

### Archive the dataset<a id="sec-3-1-2" name="sec-3-1-2"></a>

-   Visit <https://discovery.psi.ch>
-   Click 'Archive' for the selected datasets
-   The system will now copy the data to the PetaByte Archive at CSCS

### Retrieve data from the catalog<a id="sec-3-1-3" name="sec-3-1-3"></a>

-   Find the dataset on <https://discovery.psi.ch> and click 'Retrieve'
-   Wait for the data to be copied to the PSI retrieval system
-   run **datasetRetriever** script

The following sections describe the process in more detail:

## Ingestion of Datasets<a id="sec-3-2" name="sec-3-2"></a>

### Introduction and Overview<a id="sec-3-2-1" name="sec-3-2-1"></a>

First you need to define datasets. A dataset is a logical grouping of
potentially many files. It is up to the creator of the files to define
datasets from the files. When defining datasets take the following
conditions into account

-   a dataset is the smallest unit for adding meta data
-   a dataset is the smallest unit for data handling (archiving and retrieval)
-   a dataset is the smallest unit for publication (DOI assignmnet)

Therefore you need to find a compromise between putting too few or too
many files into a single dataset. 

"Ingestion" of datasets means, that you make data known to the data
catalog by providing both metadata about the dataset and the file
listing comprising the dataset. For each dataset a persistent
identifier (PID) is automatically created.

It is important to note that the data catalog is a "passive" system in
the sense that it has to be told if new data arrives. The data catalog
has no direct access to the file systems containing the actual
files. In contrast the **datasetIngestor** program is run from systems, which
have access to the data files.

The datasets belong to an so called ownerGroup. Only members of these
groups have access to the data, unless the dataset is being
published. At PSI there are two types of ownerGroups,

-   pgroups, starting with letter "p". They are used for experimental
    data linked to a proposal system. They are managed by the digital
    user office
-   a-groups, starting with "a-" for any other data to be archived

Once data is contained in the data catalog, this information is
considered to be stored permanently. However after a retention period
the connected raw data files may actually be deleted. In this case the
dataset is **marked** as deleted in the data catalog, but the data
catalog entry persists, in agreement with the FAIR principles.

Warning: you should not modify the files which make up your dataset
after the dataset was ingested to the datacatalog. This means that you
should ingest the data only, if you are sure that no further
modifications on the files take place. The subsequent archive job will
only take care of the files which existed at ingest time and otherwise
return an error message and not archive the data at all.

### Definition of input files<a id="sec-3-2-2" name="sec-3-2-2"></a>

First you need to specify the location of the files that you want to
have stored as one dataset. A typically example would be all the files
taken during a measurement, a scan etc or all output data from an
analysis of raw data files. In the simplest case it is sufficient to
define only one location. i.e. the **sourceFolder**, which should
contain all the files (and only those files) that make up the
dataset. In a more general case you can also specify an explicit list
of files and/or directories that you want to have assembled to a
dataset.

### Recommended file structure for raw datasets<a id="sec-3-2-3" name="sec-3-2-3"></a>

The recommended way of structuring your data on disk is the following:

    e12345   <--- user's group e-account, linked to a DUO proposal
    
      - sampleName  <-- contains measurement for a given sample
         - datasetfolder1   <-- name can be anything
           ... in here all the files, and only the files
           ... which make up a measurement
         - datasetfolder2   <-- name can be anything
           ... dito
         - etc...
         - derived-dataset1 (optional, for online processed data
                             name should contain "derived") 
           ... in here all the files and only the files
           ... which make up the derived data 
         - derived-dataset2
           ... dito
    
      - nextSampleName... 
    
    e12375    <--- next user's group e-account

### Definition of metadata<a id="sec-3-2-4" name="sec-3-2-4"></a>

For all methods it is necessary to collect the required metadata
information before the ingestion. There are two types of metadata
which need to be provided:

-   administrative metadata: specifies when and where the data is taken,
    who is the owner etc. There are both mandatory and optional fields
    and the fields depend on the type of the dataset
    (generic/raw/derived), see Appendix. The most important metadata
    field for ownership is the value of the "ownerGroup" field, which
    defines a group name, whose member have access to the data.
-   scientific metadata: this depends on the scientific discipline and
    can be defined in a flexible way by respective research group. It is
    up to the research groups to define the format(s) of their data that
    they want to support, ideally on an international level. It is
    strongly recommended that physical quantities are stored in the
    following format (the field names are just examples, the structure
    with the two fields "value" and "units" is important here)

    "scientificMetadata": {
           ...
           "beamlineParameters": {
               "Ring current": {
                   "value": 402.246,
                   "units": "mA"
               },
               "Beam energy": {
                   "value": 22595,
                   "units": "eV"
               }
           }
           ....
    }

In future for such quantities the data catalog will automatically add
two additional fields "valueSI" and "unitsSI" with the corresponding
SI units. The rationale for this is to support value queries in a
reliable manner across datasets with potentially different units
chosen for the same quantity:

    "scientificMetadata": {
           ...
           "beamlineParameters": {
               "Ring current": {
                   "value": 402.246,
                   "units": "mA",
                   "valueSI": 0.402246,
                   "unitsSI": "A"
               },
               "Beam energy": {
                   "value": 22595,
                   "units": "eV",
                   "valueSI": 3.6201179E-15
                   "unitsSI":"J"
               }
           }
           ....
    }

### Use Case: Manual ingest using datasetIngestor program<a id="sec-3-2-5" name="sec-3-2-5"></a>

1.  Overview

    Data owners may want to define in an adhoc manner the creation of
    datasets in order to allow a subsequent archiving of the data. The
    most important use cases are
    
    -   raw data from a beamline
    -   derived data created by a scientist
    -   archiving of historic data
    -   archiving of data stored on local (decentral) file storage systems
    
    For this purpose a command line client **datasetIngestor** is provided
    which allows to
    
    -   ingest the meta data and files
    -   optionally copy the data to a central cache file server
    
    The necessary steps to use this tool are now described:

2.  Preparation of the meta data

    You need to create a file metadata.json defining at least the
    administrative metadata

3.  Example of minimal json file for raw data:

        {
            "creationLocation": "/PSI/SLS/TOMCAT",
            "sourceFolder": "/scratch/devops",
            "type": "raw",
            "ownerGroup":"p16623"
        }

4.  Example for raw data including scientific metadata

        {
            "principalInvestigator": "egon.meier@psi.ch",
            "creationLocation": "/PSI/SLS/TOMCAT",
            "dataFormat": "Tomcat pre HDF5 format 2017",
            "sourceFolder": "/sls/X02DA/data/e12345/Data10/disk3/817b_B2_",
            "owner": "Egon Meier",
            "ownerEmail": "egon.meier@psi.ch",
            "type": "raw",
            "description": "Add  a short description here for this dataset ...",
            "ownerGroup": "p12345",
            "scientificMetadata": {
                "beamlineParameters": {
                    "Monostripe": "Ru/C",
                    "Ring current": {
                        "v": 0.402246,
                        "u": "A"
                    },
                    "Beam energy": {
                        "v": 22595,
                        "u": "eV"
                    }
                },
                "detectorParameters": {
                    "Objective": 20,
                    "Scintillator": "LAG 20um",
                    "Exposure time": {
                        "v": 0.4,
                        "u": "s"
                    }
                },
                "scanParameters": {
                    "Number of projections": 1801,
                    "Rot Y min position": {
                        "v": 0,
                        "u": "deg"
                    },
                    "Inner scan flag": 0,
                    "File Prefix": "817b_B2_",
                    "Sample In": {
                        "v": 0,
                        "u": "m"
                    },
                    "Number of darks": 10,
                    "Rot Y max position": {
                        "v": 180,
                        "u": "deg"
                    },
                    "Angular step": {
                        "v": 0.1,
                        "u": "deg"
                    },
                    "Number of flats": 120,
                    "Sample Out": {
                        "v": -0.005,
                        "u": "m"
                    },
                    "Flat frequency": 0,
                    "Number of inter-flats": 0
                }
            }
        }

5.  Example of minimal json file for derived data:

        { "sourceFolder" : "/data/test/myExampleData",
          "type" : "derived",
          "ownerGroup": "p12345",
          "investigator":"federika.marone@psi.ch",
          "inputDatasets": ["/data/test/input1.dat",
                            "20.500.11935/000031f3-0675-4d30-b5ca-b9c674bcf027"],
          "usedSoftware": ["https://gitlab.psi.ch/MyAnalysisRepo/tomcatScripts/commit/60629a1cbef493a26aac626602ba8f1a6c9e14d2"]
          }
    
    -   owner and contactEmail will be filled automatically
    -   important: in case you ingest derived datasets with a **beamline
        account** , such as slstomcat (instead of a personal account), you **have** to add the beamline account
        to the accessGroups field like this:
    
        { "sourceFolder" : "/data/test/myExampleData",
          "type" : "derived",
          "ownerGroup": "p12345",
          "accessGroups":["slstomcat"], 
          "investigator":"federika.marone@psi.ch",
          "inputDatasets": ["/data/test/input1.dat",
                            "20.500.11935/000031f3-0675-4d30-b5ca-b9c674bcf027"],
          "usedSoftware": ["https://gitlab.psi.ch/MyAnalysisRepo/tomcatScripts/commit/60629a1cbef493a26aac626602ba8f1a6c9e14d2"]
          }
    
    1.  Extended derived example
    
            {
                "sourceFolder": "/some/folder/containg/the/derived/data",
                "owner": "Thomas Meier",
                "ownerEmail": "thomas.meier@psi.ch",
                "contactEmail": "eugen.mueller@psi.ch",
                "type": "derived",
                "ownerGroup": "p13268",
                "creationTime": "2011-09-14T12:08:25.000Z",
                "investigator": "thomas.meier@psi.ch",
                "inputDatasets": [
                    "20.500.11935/000031f3-0675-4d30-b5ca-b9c674bcf027",
                    "20.500.11935/000031f3-0675-4d30-b5ca-b9c674bcf028"
                ],
                "usedSoftware": ["https://gitlab.psi.ch/MyAnalysisRepo/tomcatScripts/commit/60629a1cbef493a26aac626602ba8f1a6c9e14d2"]
            }

6.  Optionally: preparation of a file listing file

    **Please note**: The following is only needed, if you do not want to
    store all files in a source Folder, but just a **subset**. In this case
    you can specify an explicit list of files and directories. Only the
    files specified in this list will be stored as part of the
    dataset. For the directories in this list it is implied that they are
    recursively descended and all data contained in the directory is taken
    Here is an example for a filelisting.txt file. All entries in this
    textfiles are path names **relativ** to the sourceFolder specified in
    the metadata.json file
    
    Example of filelisting.txt
    
        datafile1
        datafile2
        specialStuff/logfile1.log
        allFilesInThisDirectory

7.  Optionally: for multiple datasets to be created

    If you have many sourceFolders containing data, each to be turned into
    a dataset then the easiest method is to define a 'folderlisting.txt'
    file.  (the file must have exactly this name). This is a useful option
    to archive large amounts of "historic" data.
    
    Each line in this file is the absolute path to the sourceFolder In
    this case it is assumed, that the metadata.json file is valid for all
    datasets and that **all** files inside the sourceFolder are part of the
    dataset (i.e. you can **not** combine the filelisting.txt option with the
    folderlisting.txt option)
    
    Example of folderlisting.txt
    
        /some/folder/containg/the/data/raw/sample1
        /some/folder/containg/the/data/raw/sample2
        /some/folder/containg/the/data/derived

8.  Starting the ingest

    Just run the following command in a terminal as a first test if
    everything is okay. This is a so called "dry run" and nothing will
    actually be stored, but the consistency of the data will be checked
    and the folders will be scanned for files
    
        datasetIngestor metadata.json [filelisting.txt | 'folderlisting.txt']
    
    You will be prompted for your username and password.
    
    If everything looks as expected you should now repeat the command with
    the "&#x2013;ingest" flag to actually store the dataset(s) in the data
    catalog
    
        datasetIngestor --ingest metadata.json [filelisting.txt | 'folderlisting.txt']
    
    When the job is finshed all needed metadata will be ingested into the
    data catalog (and for decentral data the data will be copied to the
    central cache file server).
    
    In addition you have the option to directly trigger the archiving of
    the data to tape by adding the &#x2013;autoarchive flag. Do this only if you
    sure that this data is worth to be archived

### Use Case: Automated ingest of raw datasets from beamline or instruments<a id="sec-3-2-6" name="sec-3-2-6"></a>

1.  General approach

    This method usually requires a fully automatic ingestion procedure,
    since data is produced at regular times and in a predictable way.
    
    For each beamline this automation is done together with the experts
    from the data catalog group and potentially with the help from the
    controls /detector-integration groups. Please contact
    scicatarchivemanager@psi.ch to get in touch.
    
    The recommended method is to define preparation scripts, which
    automatically produce the files metadata.json and optionally
    filelisting.txt or folderlisting.txt (for multiple datasets) as you
    would do in the manual case described in the previous section.
    Example of such scripts can be provided by the data catalog team,
    please contact scicatingestor@psi.ch for further help. The effort to
    implement such a system depends very much on the availability of the
    meta data as well as on the effort to convert the existing metadata to
    the data catalog format inside the converter processes. If the meta
    data is already available in some form in a file an estimate of the
    order of magnitude of work needed per instrument is 1-2 person-weeks
    of work, including test runs etc. But efforts may also be considerably
    smaller or larger in some cases.
    
    Then you run the datasetIngestor program usually under a beamline
    specic account. In order to run fully automatic all potential
    questions asked interactively by the program must be pre-answered
    through a set of command line options:
    
        datasetIngestor [options] metadata-file [filelisting-file|'folderlisting.txt']
        
          -allowexistingsource
                Defines if existing sourceFolders can be reused
          -autoarchive
                Option to create archive job automatically after ingestion
          -copy
                Defines if files should be copied from your local system to a central server before ingest.
          -devenv
                Use development environment instead of production environment (developers only)
          -ingest
                Defines if this command is meant to actually ingest data
          -linkfiles string
                Define what to do with symbolic links: (keep|delete|keepInternalOnly) (default "keepInternalOnly")
          -noninteractive
                If set no questions will be asked and the default settings for all undefined flags will be assumed
          -tapecopies int
                Number of tapecopies to be used for archiving (default 1)
          -testenv
                Use test environment (qa) instead of production environment
          -user string
                Defines optional username:password string
    
    -   here is a typical example using the MX beamline at SLS as an example
        and ingesting a singel dataset with meta data defined in
        metadata.json
    
        datasetIngestor -ingest \
          -linkfiles keepInternalOnly \
          -allowexistingsource \
          -user slsmx:XXXXXXXX \
          -noninteractive \
           metadata.json
    
    This command must be called by the respective data acquisition systems
    at a proper time, i.e. after all the files from the measurement run
    have been written to disk and all metadata became available (often
    this meta data is collected by the controls system). 

2.  HDF5 Files

    If the raw data exists in form of HDF5 files, there is a good chance
    that the meta data can be extracted from the HDF5 files' meta data. In
    such a case the meta data extraction must be done as part of the part
    beamline preparation scripts. Example of such HDF5 extraction scripts
    exist which can the basis of a beamline specific solution, again
    please contact scicatingestor@psi.ch. These scripts will mostly need
    minimal adjustments for each beamline, mainly specifying the filter
    conditions defining which of the meta data in the HDF5 file are to be
    considered meta data for the data catalog.
    
    Very often the whole dataset will only consist of one HDF5 file, thus
    also simplifying the filelisting definition.

### Use Case: Ingest datasets stored on decentral systems<a id="sec-3-2-7" name="sec-3-2-7"></a>

These are data that you want to have archived for some reason, but are
not available on central file systems. Data from the old PSI archiv
system fall in this category or data from local PCs, Laptops or
instruments. If this data is not assigned to a p-group (given via the
DUO digital user office, usually linked to a proposal) then you must
assign this data to an a-group. The allocation of an "a-group" for
this kind of data must be done beforehand by a tool currently in
preparation at AIT. The "a-group" will define the ownership and
therefor the access to the data by listing a number of users onside the
group.

Otherwise just follow the description in the section "Manual ingest
using datasetIngestor program" and use the option -copy, e.g.

    datasetIngestor -autoarchive -copy -ingest metadata.json

This command will copy the data to a central rsync server, from where
the archive system can then copy the files to tape, in this case
(option -autoarchive) the copy to archive tapes will happen automatically

On recent versions of the datasetIngestor program the program detects
automatically,if your data lies on central or decentral systems. In
the latter case it will, after a confirmation by the user, copy the
data automatically to the rsync cache server, even if the copy flag is
not provided.

### Use Case: Ingest datasets from simulations/model calculations<a id="sec-3-2-8" name="sec-3-2-8"></a>

These can be treated like datasets of type "base" or "raw". In the
latter case specify the field "creationLocation" as the name of the
server or cluster which produced the simulation files. Otherwise the
procedure is identical to the previous use case.

## Archiving of Datasets<a id="sec-3-3" name="sec-3-3"></a>

The second step is the actual archiving of the data, i.e. copying the
data to tape. For this the **archive** system has access to the data
files themselves for the purpose of storing the data on tape for
longterm storage needs. The following data is currently connected
centrally to the archive servers:

-   the online storage of the beamlines
-   the offline storage from the Ra cluster
-   the AFS file system. Here you have to follow the instructions in the
    Appendix "Instructions to set ACLS in AFS" to set the proper access rights

The data catalog system keeps track of the
files making up the dataset and instructs the archive system to store
the files as a dataset on tape, either automatically or triggered by
some user action

To trigger the archiving process a Job has to be submitted, which
usually happens via the GUI at <https://discovery.psi.ch>: select the
datasets you want to have archived from the set of archivable datasets
and then click on the archive button.

Another option is to trigger the archive job together with the ingest
job in one step as part of the command line tool datasetIngestor by
using the &#x2013;autoarchive option.

A third option is to use a command line version datasetArchiver.

    datasetArchiver [options] (ownerGroup | space separated list of datasetIds) 
    
    You must choose either an ownerGroup, in which case all archivable datasets
    of this ownerGroup not yet archived will be archived.
    Or you choose a (list of) datasetIds, in which case all archivable datasets
    of this list not yet archived will be archived.
    
    List of options:
    
      -devenv
            Use development environment instead or production
      -localenv
            Use local environment (local) instead or production
      -noninteractive
            Defines if no questions will be asked, just do it - make sure you know what you are doing
      -tapecopies int
            Number of tapecopies to be used for archiving (default 1)
      -testenv
            Use test environment (qa) instead or production
      -token string
            Defines optional API token instead of username:password
      -user string
            Defines optional username and password

## Retrieval of Datasets<a id="sec-3-4" name="sec-3-4"></a>

### Overview of Retrieval<a id="sec-3-4-1" name="sec-3-4-1"></a>

The following sections describe the retrieval via the command line
tools (please note that you can also retrieve the datasets via the 
SciCatArchiver GUI tool defined below)

-   Find the dataset on <https://discovery.psi.ch> and click 'Retrieve'
-   Wait for the data to be copied to the PSI retrieval system.
-   run datasetRetriever script to copy to final location

### More detailed description<a id="sec-3-4-2" name="sec-3-4-2"></a>

Retrieval of datasets is a two-step procedure.
In the first step you select the wanted datasets via the SciCat web
application at <https://discovery.psi.ch> . Simply select the datasets
you want to retrieve by selecting ownerGroups, time, location,
instrument name and/or keywords. Then select the retrieve button. This
will trigger a retrieve job where the selected datasets are copied to
a central cache file server pb-retrieve.psi.ch. You will be
notified by email when the retrieve is finished.


In the second step the user copies the retrieved datasets to a final
location, e.g. the work space of a data analysis cluster. (WIP: The
datasets on the cache file server will be deleted automatically after
some time.) The easiest way to do this is to use the **datasetRetriever** tool. 

    Tool to retrieve datasets from the intermediate cache server of the tape archive
    to the destination path on your local system.
    Run script with 1 argument:
    
    datasetRetriever [options] local-destination-path
    
    Per default all available datasets on the retrieve server will be fetched.
    Use option -dataset or -ownerGroup to restrict the datasets which should be fetched.
    
      -chksum
            Switch on optional chksum verification step (default no checksum tests)
      -dataset string
            Defines single dataset to retrieve (default all available datasets)
      -devenv
            Use development environment (default is to use production system)
      -ownergroup string
            Defines to fetch only datasets of the specified ownerGroup (default is to fetch all available datasets)
      -retrieve
            Defines if this command is meant to actually copy data to the local system (default nothing is done)
      -testenv
            Use test environment (qa) (default is to use production system)
      -token string
            Defines optional API token instead of username:password
      -user string
            Defines optional username and password (default is to prompt for username and password)

For the program to check which data is available on the cache server
and if the catalog knows about these datasets, you can use:

    datasetRetriever my-local-destination-folder
    
    ======Checking for available datasets on archive cache server ebarema4in.psi.ch:
    
    Dataset ID                            Size[MB]  Owner SourceFolder
    ===================================================================
    0f6fe8b3-d3f1-4cfb-a1af-0464c901a24f      1895 p16371 /sls/MX/Data10/e16371/20171017_E2/cbfs/2017-10-17_22-28-30_Na108_thau7_100degs_dtz60_f_500_Hz_Eth0_6200_eV
    58f2037e-3f9b-4e08-8963-c70c3d29c068      1896 p16371 /sls/MX/Data10/e16371/20171017_E2/cbfs/2017-10-17_21-41-02_cca385a_lyso8_100degs_f_500_Hz_Eth0_6200_eV
    cf8e5b25-9c76-49a7-80d9-fd38a71e0ef8      3782 p16371 /sls/MX/Data10/e16371/20171017_E2/cbfs/2017-10-18_10-15-41_na108_thau6_50degs_lowdose_pos1_f_500_Hz_Eth0_6200_eV
    df1c7a17-2caa-41ee-af6e-c3cf4452af17      1893 p16371 /sls/MX/Data10/e16371/20171017_E2/cbfs/2017-10-17_20-58-34_cca385a_lyso3_100degs_f_500_Hz_Eth0_6200_eV

If you want you can skip the previous step and
directly trigger the file copy by adding the -retrieve flag:

    datasetRetriever -retrieve <local destinationFolder>

This will copy the files into the destinationFolder using the original
sourceFolder path beneath the destinationFolder. This is especially
useful if you want to retrieve many datasets, which you expect to
appear in the same folder structure as originally. 

Optionally you can also verify the consistency of the copied data by
using the -chksum flag

    datasetRetriever -retrieve -chksum <local destinationFolder>

If you just want to retrieve a single dataset do the following:

    datasetRetriever -retrieve -dataset <datasetId> <local destinationFolder>

If you want to retrieve all datasets of a given **ownerGroup** do the following:

    datasetRetriever -retrieve -ownergroup <group> <local destinationFolder>

### Expert commands<a id="sec-3-4-3" name="sec-3-4-3"></a>

If you prefer to have more control over the file transfer you are free
to type your own rsync commands, e.g. to simply the folders available
in the retrieve cache do:

    rsync -e ssh --list-only pb-retrieve.psi.ch:retrieve/

To actually copy the data over use:

    rsync -e ssh -av pb-retrieve.psi.ch:retrieve/{shortDatasetId} your-destination-target/

In this case the shortDatsetId is the dataseid id without the PSI
prefix, e.g. for dataset PID
20.500.11935/08bc2944-e09e-48da-894d-0c5c47977553 the shortDatasetId
is 08bc2944-e09e-48da-894d-0c5c47977553

## Analyzing Metadata Statistics<a id="sec-3-5" name="sec-3-5"></a>

### Overview<a id="sec-3-5-1" name="sec-3-5-1"></a>

It is possible to analyze the information about datasets amd jobs etc,
e.g. for statistical purposes. A Jupyterhub based solution was chosen
as a tool for allowing to do this analysis in a flexible and
interactive manner. This means you can use Jupyter notebooks to query
the Data catalog via the API for its data and analyze the results in
terms of tables and graphs. Example notebooks are provided.

### Getting started<a id="sec-3-5-2" name="sec-3-5-2"></a>

Simply follow the following link and login with your PSI account:
<https://jupyterhub.apps.ocp4a.psi.ch/> .  The initial start of the
Jupyter environments takes some time (about 40 seconds), but
subsequent starts are much faster. You will then see a "bootstrap"
notebook which you can execute to populate your Jupyter home directory
with the example notebooks. 

The example notebooks require you to login to the data catalog API
server. Here you can again use your personal account, which gives you
access to all data, for which you have read access (i.e. for which you
are member of the associated p-group). Beamline managers can also use
the beamline accounts here in order to get the statistics relevant for
the whole beamline. You can then look at the example notebooks,
e.g. datasetAnalyzer.ipynb and run it, look at resulting tables and
graphs. Afterwards you can optionally adapt the notebooks to your
needs.

Please note, that this service is currently only available as a pilot
with **no guaranteed availability**. This also means, that you should
make **regular backups of your own notebooks** which you may develop
using this tool. For this you can e.g. simply download the notebook
and copy it to a place for which backup exists, like your home
directory.

# Graphical desktop user interface - SciCat<a id="sec-4" name="sec-4"></a>

## General considerations<a id="sec-4-1" name="sec-4-1"></a>

`SciCat` is a GUI based tool designed to make initial
ingests easy. It is especially useful, to ingest data, which can not
be ingested automatically. Therefore it is designed in particular to
assist you when archiving derived datasets. Often, the archival of
derived data cannot be scheduled in advance, nor does it follow a
strict file structure. The `SciCat` GUI can help you to ingest such
datasets more easily. Yet, the ingestion of raw datasets is also
supported.  Additionally, the tool also allows for the convenient
retrieval of datasets.

## Getting started<a id="sec-4-2" name="sec-4-2"></a>

Currently, `SciCat` is supported on PSI-hosted **Linux** and **Windows**
systems and is accessible on the Ra cluster as part of the datacatalog
module: just type

    module load datacatalog

Then the software can be started with

    SciCat

On the SLS beamline consoles the software is also pre-installed in the
/work/sls/bin folder, which is part of the standard PATH variable. 

If you are not working on the Ra cluster you can download the
software on Linux:

    /usr/bin/curl -O https://gitlab.psi.ch/scicat/tools/raw/master/linux/SciCat;chmod +x ./SciCat

On Windows the executable can be downloaded from
<https://gitlab.psi.ch/scicat/tools/-/blob/master/windows/SciCat.exe>

## Login and permissions<a id="sec-4-3" name="sec-4-3"></a>

After starting the GUI, you will be asked for a username and password. Please 
enter your PSI credentials. Functional accounts are not supported. 

## Pgroup selection<a id="sec-4-4" name="sec-4-4"></a>

The first step is always to select the pgroup. If there is no proposal assigned to 
this account, you will have to specify the information about the PI manually. 

![img](./screenshots/proposal_found.png "Pgroup selection")

## Archiving<a id="sec-4-5" name="sec-4-5"></a>

After selection the files, you will be prompted with a metadata editor, where you can modify 
the general info, such as dataset name, description etc. Please make 
sure that you select the correct data type (raw or derived). As a general rule of thumb, it is 
a derived dataset if you can specify a raw dataset as input. If you want to ingest a derived dataset, 
you can specify corresponding raw datasets on the "Input datasets" tab. 
To edit scientific metadata, switch to "Scientific metadata" tab. 

## Retrieval<a id="sec-4-6" name="sec-4-6"></a>

Retrieving successfully archived datasets from SciCat is a two-step process. First you will have to 
retrieve to an intermediate server. Once the data is there, you will be notified by email. 
The final step is to copy the data to the final destination on your machine. 
Both steps can be steered from within the GUI. 

On the retrieve page, all datasets of your pgroup are listed. If the data has been archived successfully, 
the cell in column "retrievable" is set to "true". To retrieve the data to the intermediate file server,
select the datasets that you want to retrieve and click on "Retrieve." After the retrieval, the column
"retrieved" is set to true. You are now able to start copying the data to you local machine by selecting
the desired datasets and clicking on "Save." 

## Settings<a id="sec-4-7" name="sec-4-7"></a>

Additional settings, such as the default value for certain fields can be modified in settings panel (button
on the lower left corner). 

# Policy settings and email notifications<a id="sec-5" name="sec-5"></a>

The archiving process can further be configured via **policy**
parameters (WIP), e.g. if you require a second tape copy for very
precious data. Also the details about the notification settings by
email for both archive and retrieve jobs can be set here. You reach
the menu to set the policy values via the submenu "Archive Settings"
in the dropdown menu to the top right of the GUI.

Emails are automatically sent at the start of every archive and
retrieve jobs as well as when the job finishes. The email is sent to
the person creating the jobs. In addition it is sent the list of
emails defined in the policy settings. Per default this list is empty
but can be extended by you. In the policy one can also switch off the
email notification. However emails about error conditions (which can
be either user caused or system caused) can not be switched off. Such
error messages are always sent to the user as well as the archive
administrators.

For user caused errors the user has to take action to repair the
situation. Typically error cases are, that the user has moved or
removed part or all of the files before archiving them. System errors
on the other hand have their reason inside the catalog and archive
system (e.g. a network connection problem or similar) and will be
taken care of by the archive managers. In such a case the user
creating the job will be informed manually, when the problem is fixed
again.

# Publication of Data<a id="sec-6" name="sec-6"></a>

As part of a publication workflow datasets must become citable via a
digital object identifier (DOI). This assignment is done as part of
the publication workflow described below. The publication then can
link to these published datasets using this DOI. The DOIs can link to
both raw and/or derived datasets. The published data and therefore the
DOI ususally refers to a **set** of Datasets, thus avoiding the need to
list potentially thousands of individual dataset identifiers in a
journal publication.

You publish data in the following way: go to <https://discovery.psi.ch> ,
login and select all the datasets, that you want to publish under a
new DOI. 

![img](./screenshots/PublishingData1.png "Selecting Datasets to be published")

Then you add these datasest a a "shopping cart" by using the "add to
Cart" button. You can repeat this often as needed. Once finished with
the selection you can "check out" the cart (click on the cart in the
top bar) and pick the "Publish" action.

![img](./screenshots/PublishingData2.png "Check out cart")

This opens a form
with prefilled information derived from the connected proposal
data. This data can then be edited by the user and finally saved.

![img](./screenshots/PublishingData3.png "Defining metadata of published data")

This defines the data as to be published and makes it known to the
data catalog, but the corresponding DOI is not yet made globally
available. For this last step to happen, someone with access to this
newly generated published data definition (e.g. the person defining
the published data or e.g. the PI) has to hit the "register"
button. This will trigger the global publication of the DOI.

All published data definitions are then openly available via the so
called "Landing Pages", which are hosted on <https://doi.psi.ch> .

For now all publication are triggered by a scientist
explicitly, whenever necessary. In future in addition an automated
publication after the embargo period (default 3 years after data
taking) will be implemented (details to be defined)

# Policy Parameters<a id="sec-7" name="sec-7"></a>

Policy parameters can be defined at site level or at ownerGroup
level. For each ownerGroup at least one manager must be defined
(e.g. a principal investigator (PI) via the linked proposal
information) in the policy model (field "manager") . Only the manager
can change the policy settings at ownerGroup level, but all group
mebers can see them.

Changes to this policy settings only effect future dataset creation
and archiving

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">Parameter</th>
<th scope="col" class="left">Allowed Values</th>
<th scope="col" class="left">Default</th>
<th scope="col" class="left">Level</th>
</tr>
</thead>

<tbody>
<tr>
<td class="left">policyPublicationShiftInYears</td>
<td class="left">small positive integer, e.g. 3</td>
<td class="left">3</td>
<td class="left">Site (ro)</td>
</tr>


<tr>
<td class="left">policyRetentionShiftInYears</td>
<td class="left">small positive integer, e.g. 10</td>
<td class="left">10</td>
<td class="left">Site (ro)</td>
</tr>
</tbody>

<tbody>
<tr>
<td class="left">autoArchive</td>
<td class="left">true/false</td>
<td class="left">false</td>
<td class="left">ownerGroup</td>
</tr>


<tr>
<td class="left">tapeRedundancy</td>
<td class="left">low/medium/(high)</td>
<td class="left">low</td>
<td class="left">ownerGroup</td>
</tr>


<tr>
<td class="left">archiveEmailNotification</td>
<td class="left">true/false</td>
<td class="left">false</td>
<td class="left">ownerGroup</td>
</tr>


<tr>
<td class="left">archiveEmailsToBeNotified</td>
<td class="left">Array of additional emails</td>
<td class="left">[]</td>
<td class="left">ownerGroup</td>
</tr>


<tr>
<td class="left">retrieveEmailNotification</td>
<td class="left">true/false</td>
<td class="left">false</td>
<td class="left">ownerGroup</td>
</tr>


<tr>
<td class="left">retrieveEmailsToBeNotified</td>
<td class="left">Array of additional emails</td>
<td class="left">[]</td>
<td class="left">ownerGroup</td>
</tr>


<tr>
<td class="left">(archiveDelayInDays)</td>
<td class="left">small positive integer, e.g. 7</td>
<td class="left">0</td>
<td class="left">ownerGroup</td>
</tr>
</tbody>
</table>

The job Initiator always gets an email unless email notification is disabled.

# Appendix<a id="sec-8" name="sec-8"></a>

## Access to the API (for script developers)<a id="sec-8-1" name="sec-8-1"></a>

The data catalog can also be accessed directly via a REST API. There
exists an API "Explorer" which allows to test such API calls
conveniently. The explorer can be found at
<https://dacat-qa.psi.ch/explorer> .The explorer works with a test
database which is separate from the production database and contains
other data.

For most of the API calls you will need an access token first. You
create such an access token by "login" to the data catalog via the
following curl command:

    # for "functional" accounts
    curl -X POST --header 'Content-Type: application/json'  -d '{"username":"YOUR-LOGIN","password":"YOUR-PASSWORD"}' 'https://dacat-qa.psi.ch/api/v3/Users/login'
    
    # for normal user accounts
    curl -X POST --header 'Content-Type: application/json'  -d '{"username":"YOUR-LOGIN","password":"YOUR-PASSWORD"}' 'https://dacat-qa.psi.ch/auth/msad'
    
    # reply if succesful:
    {"id":"NQhe3...","ttl":1209600,"created":"2019-01-22T07:03:21.422Z","userId":"5a745bde4d12b30008020843"}

The "id" field contains the access token, which you copy in to the corresponding field at the top of the explorer page.

Afterwards you can test the full API. If you found the right API call
you can finally apply the call to the production system by replacing
"dacat-qa" by "dacat" and then by retrieving the access token from the
production system.

## Using datasetIngestor inside wrapper scripts (for developers)<a id="sec-8-2" name="sec-8-2"></a>

The command datasetIngestor returns with a return code equal zero in
case the command could be executed succesfully. If the program however
fails for some reason the return code will be one. Typical examples of
failures are that files can not be found or not be accessed. Other
possibilities are that the catalog system is not available,
e.g. during scheduled maintenance periods. All outputs describing the
reason for the failure are written to STDERR. Please have a look at
these outputs to understand what the reason for the failure was. If
you need help please contact scicatingestor@psi.ch

Please note: it is the task of the wrapper scripts to test
for the return code and to repeat the command once all conditions for
a succesful execution are fulfilled

In case the ingest finishes succesfully the dataset persistent
identifiers (PID) of the resulting dataset(s) are written to STDOUT,
one line per dataset.

## Ingestion of datasets which should never be published<a id="sec-8-3" name="sec-8-3"></a>

For datasets which should never be published you should add the
following fields at ingest time to your metadata.json file:

    "datasetlifecycle": {
        "publishable":false,
        "dateOfPublishing":"2099-12-31T00:00:00.000Z",
        "archiveRetentionTime":"2099-12-31T00:00:00.000Z"
    }

-   this will move the time of publication to a date in some far future
    (2100 in this case)

## Retrieving proposal information<a id="sec-8-4" name="sec-8-4"></a>

In case you need information about the principal investigator you can
use the command datasetGetProposal, which returns the proposal
information for a given ownerGroup

    /usr/bin/curl -O https://gitlab.psi.ch/scicat/tools/raw/master/linux/datasetGetProposal;chmod +x ./datasetGetProposal

## Link to Group specific descriptions<a id="sec-8-5" name="sec-8-5"></a>

-   BIO department: <https://intranet.psi.ch/BIO/ComputingDataCatalog>

## Metadata Field definitions<a id="sec-8-6" name="sec-8-6"></a>

The following table defines the mandatory and optional fields for the
administrative metadata, which have to be provided (status January
2019). All fields marked "m" are mandatory, the rest is optional. Some
fields are filled automatically if possible, see comments. For the
most recent status see this URL
<https://scicatproject.github.io/api-documentation/> and follow the link
called "Model" for the respective datamodel (e.g. Dataset), visible
e.g. inside the GET API call section.

All "Date" fields must follow the date/time format defined in RFC
3339, section 5.6, see <https://www.ietf.org/rfc/rfc3339.txt>

### Metadata field definitions for datasets of type "base"<a id="sec-8-6-1" name="sec-8-6-1"></a>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">field</th>
<th scope="col" class="left">type</th>
<th scope="col" class="left">must</th>
<th scope="col" class="left">comment</th>
</tr>
</thead>

<tbody>
<tr>
<td class="left">pid</td>
<td class="left">string</td>
<td class="left">m</td>
<td class="left">filled by API automatically, do **not** provide this</td>
</tr>


<tr>
<td class="left">owner</td>
<td class="left">string</td>
<td class="left">m</td>
<td class="left">filled by datasetIngestor if missing</td>
</tr>


<tr>
<td class="left">ownerEmail</td>
<td class="left">string</td>
<td class="left">&#xa0;</td>
<td class="left">filled by datasetIngestor if missing</td>
</tr>


<tr>
<td class="left">orcidOfOwner</td>
<td class="left">string</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">contactEmail</td>
<td class="left">string</td>
<td class="left">m</td>
<td class="left">filled by datasetIngestor if missing</td>
</tr>


<tr>
<td class="left">datasetName</td>
<td class="left">string</td>
<td class="left">&#xa0;</td>
<td class="left">set to "tail" of sourceFolder path if missing</td>
</tr>


<tr>
<td class="left">sourceFolder</td>
<td class="left">string</td>
<td class="left">m</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">size</td>
<td class="left">number</td>
<td class="left">&#xa0;</td>
<td class="left">autofilled when OrigDataBlock created</td>
</tr>


<tr>
<td class="left">packedSize</td>
<td class="left">number</td>
<td class="left">&#xa0;</td>
<td class="left">autofilled when DataBlock created</td>
</tr>


<tr>
<td class="left">creationTime</td>
<td class="left">date</td>
<td class="left">m</td>
<td class="left">filled by API if missing</td>
</tr>


<tr>
<td class="left">type</td>
<td class="left">string</td>
<td class="left">m</td>
<td class="left">(raw, derived&#x2026;)</td>
</tr>


<tr>
<td class="left">validationStatus</td>
<td class="left">string</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">keywords</td>
<td class="left">Array[string]</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">description</td>
<td class="left">string</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">classification</td>
<td class="left">string</td>
<td class="left">&#xa0;</td>
<td class="left">filled by API or datasetIngestor if missing</td>
</tr>


<tr>
<td class="left">license</td>
<td class="left">string</td>
<td class="left">&#xa0;</td>
<td class="left">filled by datasetIngestor if missing (CC By-SA 4.0)</td>
</tr>


<tr>
<td class="left">version</td>
<td class="left">string</td>
<td class="left">&#xa0;</td>
<td class="left">autofilled by API</td>
</tr>


<tr>
<td class="left">doi</td>
<td class="left">string</td>
<td class="left">&#xa0;</td>
<td class="left">filled as part of publication workflow</td>
</tr>


<tr>
<td class="left">isPublished</td>
<td class="left">boolean</td>
<td class="left">&#xa0;</td>
<td class="left">filled by datasetIngestor if missing (false)</td>
</tr>


<tr>
<td class="left">ownerGroup</td>
<td class="left">string</td>
<td class="left">m</td>
<td class="left">must be filled explicitly</td>
</tr>


<tr>
<td class="left">accessGroups</td>
<td class="left">Array[string]</td>
<td class="left">&#xa0;</td>
<td class="left">filled by datasetIngestor to beamline specific group</td>
</tr>


<tr>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
<td class="left">derived from creationLocation</td>
</tr>


<tr>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
<td class="left">e.g. /PSI/SLS/TOMCAT -> accessGroups=["slstomcat"]</td>
</tr>
</tbody>
</table>

### Additional fields for type="raw"<a id="sec-8-6-2" name="sec-8-6-2"></a>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">field</th>
<th scope="col" class="left">type</th>
<th scope="col" class="left">must</th>
<th scope="col" class="left">comment</th>
</tr>
</thead>

<tbody>
<tr>
<td class="left">principalInvestigator</td>
<td class="left">string</td>
<td class="left">m</td>
<td class="left">filled in datasetIngestor if missing (proposal must exist)</td>
</tr>


<tr>
<td class="left">endTime</td>
<td class="left">date</td>
<td class="left">&#xa0;</td>
<td class="left">filled from datasetIngetor if missing</td>
</tr>


<tr>
<td class="left">creationLocation</td>
<td class="left">string</td>
<td class="left">m</td>
<td class="left">see known Instrument list below</td>
</tr>


<tr>
<td class="left">dataFormat</td>
<td class="left">string</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">scientificMetadata</td>
<td class="left">object</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
</tr>
</tbody>
</table>

### Additional fields for type="derived"<a id="sec-8-6-3" name="sec-8-6-3"></a>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">field</th>
<th scope="col" class="left">type</th>
<th scope="col" class="left">must</th>
<th scope="col" class="left">comment</th>
</tr>
</thead>

<tbody>
<tr>
<td class="left">investigator</td>
<td class="left">string</td>
<td class="left">m</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">inputDatasets</td>
<td class="left">Array[string]</td>
<td class="left">m</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">usedSoftware</td>
<td class="left">string</td>
<td class="left">m</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">jobParameters</td>
<td class="left">object</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">jobLogData</td>
<td class="left">string</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">scientificMetadata</td>
<td class="left">object</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
</tr>
</tbody>
</table>

## Instructions to set ACLS in AFS<a id="sec-8-7" name="sec-8-7"></a>

In the AFS file system the user have to permit access to the
sourceFolder by setting read and lookup ACL permission for the AFS
group pb-archive. The easiest way to achieve is to run the following
script with the sourceFolder as an argunent

    /afs/psi.ch/service/bin/pb_setacl.sh sourceFolder

This script must be run by a person who has the rights to modify the
access rights in AFS.

## List of known creationLocation for raw data<a id="sec-8-8" name="sec-8-8"></a>

The following values for the creationLocation should be used for the
respective beamlines. They are derived from the identifiers used
inside the digital user office DUO

## SLS<a id="sec-8-9" name="sec-8-9"></a>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">Beamline</th>
<th scope="col" class="left">creationLocation</th>
<th scope="col" class="left">Ingest Account</th>
<th scope="col" class="left">Email Distribution</th>
</tr>
</thead>

<tbody>
<tr>
<td class="left">Adress-RIXS</td>
<td class="left">/PSI/SLS/ADRESS-RIXS</td>
<td class="left">slsadress-rixs</td>
<td class="left">slsadress-rixs@psi.ch</td>
</tr>


<tr>
<td class="left">Adress-SX-ARPES</td>
<td class="left">/PSI/SLS/ADRESS-SX-ARPES</td>
<td class="left">slsadress-sx-arpes</td>
<td class="left">slsadress-sx-arpes@psi.ch</td>
</tr>


<tr>
<td class="left">cSAXS</td>
<td class="left">/PSI/SLS/CSAXS</td>
<td class="left">slscsaxs</td>
<td class="left">slscsaxs@psi.ch</td>
</tr>


<tr>
<td class="left">Micro-XAS</td>
<td class="left">/PSI/SLS/MICRO-XAS</td>
<td class="left">slsmicro-xas</td>
<td class="left">slsmicro-xas@psi.ch</td>
</tr>


<tr>
<td class="left">Micro-XAS-Femto</td>
<td class="left">/PSI/SLS/MICRO-XAS-FEMTO</td>
<td class="left">slsmicro-xas-femto</td>
<td class="left">slsmicro-xas-femto@psi.ch</td>
</tr>


<tr>
<td class="left">MS-Powder</td>
<td class="left">/PSI/SLS/MS-POWDER</td>
<td class="left">slsms-powder</td>
<td class="left">slsms-powder@psi.ch</td>
</tr>


<tr>
<td class="left">MS-Surf-Diffr</td>
<td class="left">/PSI/SLS/MS-SURF-DIFFR</td>
<td class="left">slsms-surf-diffr</td>
<td class="left">slsms-surf-diffr@psi.ch</td>
</tr>


<tr>
<td class="left">Nano-XAS</td>
<td class="left">/PSI/SLS/NANOXAS</td>
<td class="left">slsnanoxas</td>
<td class="left">slsnanoxas@psi.ch</td>
</tr>


<tr>
<td class="left">Pearl</td>
<td class="left">/PSI/SLS/PEARL</td>
<td class="left">slspearl</td>
<td class="left">slspearl@psi.ch</td>
</tr>


<tr>
<td class="left">Phoenix</td>
<td class="left">/PSI/SLS/PHOENIX</td>
<td class="left">slsphoenix</td>
<td class="left">slsphoenix@psi.ch</td>
</tr>


<tr>
<td class="left">Pollux</td>
<td class="left">/PSI/SLS/POLLUX</td>
<td class="left">slspollux</td>
<td class="left">slspollux@psi.ch</td>
</tr>


<tr>
<td class="left">MX (PX,PXII,PXIII)</td>
<td class="left">/PSI/SLS/MX</td>
<td class="left">slsmx</td>
<td class="left">slsmx@psi.ch</td>
</tr>


<tr>
<td class="left">SIM</td>
<td class="left">/PSI/SLS/SIM</td>
<td class="left">slssim</td>
<td class="left">slssim@psi.ch</td>
</tr>


<tr>
<td class="left">Sis-Cophee</td>
<td class="left">/PSI/SLS/SIS-COPHEE</td>
<td class="left">slssis-cophee</td>
<td class="left">slssis-cophee@psi.ch</td>
</tr>


<tr>
<td class="left">Sis-Hrpes</td>
<td class="left">/PSI/SLS/SIS-HRPES</td>
<td class="left">slssis-hrpes</td>
<td class="left">slssis-hrpes@psi.ch</td>
</tr>


<tr>
<td class="left">Super-XAS</td>
<td class="left">/PSI/SLS/SUPER-XAS</td>
<td class="left">slssuper-xas</td>
<td class="left">slssuper-xas@psi.ch</td>
</tr>


<tr>
<td class="left">Tomcat</td>
<td class="left">/PSI/SLS/TOMCAT</td>
<td class="left">slstomcat</td>
<td class="left">slstomcat@psi.ch</td>
</tr>


<tr>
<td class="left">VUV</td>
<td class="left">/PSI/SLS/VUV</td>
<td class="left">slsvuv</td>
<td class="left">slsvuv@psi.ch</td>
</tr>


<tr>
<td class="left">XIL-II</td>
<td class="left">/PSI/SLS/XIL-II</td>
<td class="left">slsxil-ii</td>
<td class="left">slsxil-ii@psi.ch</td>
</tr>


<tr>
<td class="left">Xtreme</td>
<td class="left">/PSI/SLS/XTREME</td>
<td class="left">slsxtreme</td>
<td class="left">slsxtreme@psi.ch</td>
</tr>
</tbody>
</table>

## Swissfel<a id="sec-8-10" name="sec-8-10"></a>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">Beamline</th>
<th scope="col" class="left">creationLocation</th>
<th scope="col" class="left">Ingest Account</th>
<th scope="col" class="left">Email Distribution</th>
</tr>
</thead>

<tbody>
<tr>
<td class="left">Alvra</td>
<td class="left">/PSI/SWISSFEL/ARAMIS-ALVRA</td>
<td class="left">swissfelaramis-alvra</td>
<td class="left">swissfelaramis-alvra@psi.ch</td>
</tr>


<tr>
<td class="left">Bernina</td>
<td class="left">/PSI/SWISSFEL/ARAMIS-BERNINA</td>
<td class="left">swissfelaramis-bernina</td>
<td class="left">swissfelaramis-bernina@psi.ch</td>
</tr>


<tr>
<td class="left">Cristallina</td>
<td class="left">/PSI/SWISSFEL/ARAMIS-CRISTALLINA</td>
<td class="left">swissfelaramis-cristallina</td>
<td class="left">&#xa0;</td>
</tr>
</tbody>
</table>

## SINQ<a id="sec-8-11" name="sec-8-11"></a>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">Instrument</th>
<th scope="col" class="left">creationLocation</th>
<th scope="col" class="left">Ingest Account</th>
<th scope="col" class="left">Email Distribution</th>
</tr>
</thead>

<tbody>
<tr>
<td class="left">AMOR</td>
<td class="left">/PSI/SINQ/AMOR</td>
<td class="left">sinqamor</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">DMC</td>
<td class="left">/PSI/SINQ/DMC</td>
<td class="left">sinqdmc</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">EIGER</td>
<td class="left">/PSI/SINQ/EIGER</td>
<td class="left">sinqeiger</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">FOCUS</td>
<td class="left">/PSI/SINQ/FOCUS</td>
<td class="left">sinqfocus</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">HRPT</td>
<td class="left">/PSI/SINQ/HRPT</td>
<td class="left">sinqhrpt</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">ICON</td>
<td class="left">/PSI/SINQ/ICON</td>
<td class="left">sinqicon</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">Morpheus</td>
<td class="left">/PSI/SINQ/MORPHEUS</td>
<td class="left">sinqmorpheus</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">NARZISS</td>
<td class="left">/PSI/SINQ/NARZISS</td>
<td class="left">sinqnarziss</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">NEUTRA</td>
<td class="left">/PSI/SINQ/NEUTRA</td>
<td class="left">sinqneutra</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">POLDI</td>
<td class="left">/PSI/SINQ/POLDI</td>
<td class="left">sinqpoldi</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">RITA-II</td>
<td class="left">/PSI/SINQ/RITA-II</td>
<td class="left">sinqrita-ii</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">SANS-I</td>
<td class="left">/PSI/SINQ/SANS-I</td>
<td class="left">sinqsans-i</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">SANS-II</td>
<td class="left">/PSI/SINQ/SANS-II</td>
<td class="left">sinqsans-ii</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">TASP</td>
<td class="left">/PSI/SINQ/TASP</td>
<td class="left">sinqtasp</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">ZEBRA</td>
<td class="left">/PSI/SINQ/ZEBRA</td>
<td class="left">sinqzebra</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
</tr>
</tbody>
</table>

## SmuS<a id="sec-8-12" name="sec-8-12"></a>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">Instrument</th>
<th scope="col" class="left">creationLocation</th>
<th scope="col" class="left">Ingest Account</th>
<th scope="col" class="left">Email Distribution</th>
</tr>
</thead>

<tbody>
<tr>
<td class="left">Dolly</td>
<td class="left">/PSI/SMUS/DOLLY</td>
<td class="left">smusdolly</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">GPD</td>
<td class="left">/PSI/SMUS/GPD</td>
<td class="left">smusgpd</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">GPS</td>
<td class="left">/PSI/SMUS/GPS</td>
<td class="left">smusgps</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">HAL-9500</td>
<td class="left">/PSI/SMUS/HAL-9500</td>
<td class="left">smushal-9500</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">LEM</td>
<td class="left">/PSI/SMUS/LEM</td>
<td class="left">smuslem</td>
<td class="left">&#xa0;</td>
</tr>


<tr>
<td class="left">LTF</td>
<td class="left">/PSI/SMUS/LTF</td>
<td class="left">smusltf</td>
<td class="left">&#xa0;</td>
</tr>
</tbody>
</table>

## Size limitations<a id="sec-8-13" name="sec-8-13"></a>

-   a single dataset should currently not have more than 200k files
-   a single dataset should not be larger than 50 TB
-   recommended size of a single dataset: between 1GB and 1TB

# Troubleshooting<a id="sec-9" name="sec-9"></a>

## Locale error message<a id="sec-9-1" name="sec-9-1"></a>

If you get error messages like the following (so far only happened
from Mac Computers)

    perl: warning: Setting locale failed.
    perl: warning: Please check that your locale settings:
    ....

then you need to prevent that the Mac ssh client sends the
LC<sub>TYPE</sub> variable. Just follow the description in:
<https://www.cyberciti.biz/faq/os-x-terminal-bash-warning-setlocale-lc_ctype-cannot-change-locale/>

# Update History of Ingest Manual<a id="sec-10" name="sec-10"></a>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">Date</th>
<th scope="col" class="left">Updates</th>
</tr>
</thead>

<tbody>
<tr>
<td class="left">10. September 2018</td>
<td class="left">Initial Release</td>
</tr>


<tr>
<td class="left">6. October 2018</td>
<td class="left">Added warning section to not modify data after ingest</td>
</tr>


<tr>
<td class="left">10. October 2018</td>
<td class="left">ownerGroup field must be defined explicitly</td>
</tr>


<tr>
<td class="left">28. October 2018</td>
<td class="left">Added section on datasetRetriever tool</td>
</tr>


<tr>
<td class="left">20. November 2018</td>
<td class="left">Remove ssh key handling description (use Kerberos)</td>
</tr>


<tr>
<td class="left">3. December 2018</td>
<td class="left">Restructure archive stepp, add autoarchive flag</td>
</tr>


<tr>
<td class="left">17. January 2019</td>
<td class="left">Update on automatically filled values, more options for datasetIngestor</td>
</tr>


<tr>
<td class="left">22. January 2019</td>
<td class="left">Added description for API access for script developers, 2 new commands</td>
</tr>


<tr>
<td class="left">&#xa0;</td>
<td class="left">datasetArchiver and datasetGetProposal</td>
</tr>


<tr>
<td class="left">22. February 2019</td>
<td class="left">Added known beamlines(instruments (creationLocation) value list</td>
</tr>


<tr>
<td class="left">24. February 2019</td>
<td class="left">datasetIngestor use cases for automated ingests using beamline accounts</td>
</tr>


<tr>
<td class="left">23. April 2019</td>
<td class="left">Added AFS infos and available central storage, need for Kerberos tickets</td>
</tr>


<tr>
<td class="left">23. April 2019</td>
<td class="left">Availability of commands on RA cluster via pmodules</td>
</tr>


<tr>
<td class="left">3. May 2019</td>
<td class="left">Added size limitation infos</td>
</tr>


<tr>
<td class="left">9. May 2019</td>
<td class="left">Added hints for accessGroups definition for derived data</td>
</tr>


<tr>
<td class="left">&#xa0;</td>
<td class="left">Added infos about email notifications</td>
</tr>


<tr>
<td class="left">10. May 2019</td>
<td class="left">Added ownerGroup filtered retrieve option, decentral case auto detect</td>
</tr>


<tr>
<td class="left">7. Juni 2019</td>
<td class="left">Feedback from Manuel added</td>
</tr>


<tr>
<td class="left">21. Oct 2019</td>
<td class="left">New version of CLI tools to deal with edge cases (blanks in sourcefolder</td>
</tr>


<tr>
<td class="left">&#xa0;</td>
<td class="left">dangling links, ingest for other person, need for kerberos ticket as user)</td>
</tr>


<tr>
<td class="left">14. November 2019</td>
<td class="left">Restructuring of manual,New CLI tools, auto kinit login</td>
</tr>


<tr>
<td class="left">&#xa0;</td>
<td class="left">Progress indicators, chksum test updated</td>
</tr>


<tr>
<td class="left">20. Januar 2020</td>
<td class="left">Auto fill principalInvestigator if missing</td>
</tr>


<tr>
<td class="left">3. March 2020</td>
<td class="left">Added Jupyter notebook analysis section</td>
</tr>


<tr>
<td class="left">5. March 2020</td>
<td class="left">Add hint for datasets not to be published</td>
</tr>


<tr>
<td class="left">19. March 2020</td>
<td class="left">Added hint that analysis Jupyter tool is in pilot phase only</td>
</tr>


<tr>
<td class="left">19. March 2020</td>
<td class="left">Added recommendation concerning unit handling for physical quantities</td>
</tr>


<tr>
<td class="left">9. July 2020</td>
<td class="left">Added GUI tool SciCatArchiver (developer: Klaus Wakonig)</td>
</tr>


<tr>
<td class="left">11. July 2020</td>
<td class="left">Installation of SciCatArchiver on non-Ra system</td>
</tr>


<tr>
<td class="left">14. July 2020</td>
<td class="left">Added publication workflow and recommended file structure chapter</td>
</tr>


<tr>
<td class="left">16. July 2020</td>
<td class="left">Updated SciCat GUI deployment information</td>
</tr>


<tr>
<td class="left">31. July 2020</td>
<td class="left">New deploy location, + policy parameters, new recommended file structure</td>
</tr>


<tr>
<td class="left">27. August 2020</td>
<td class="left">Added Windows Support information</td>
</tr>


<tr>
<td class="left">10. Sept 2020</td>
<td class="left">Corrected example JSON syntax in one location</td>
</tr>
</tbody>
</table>